{"/Docker/Build-Container-Image":{"title":"Build Container Image","data":{"":"Create a Dockerfile: Start by creating a text file called \"Dockerfile\" (with no file extension) in the root directory of your project.\nChoose a base image: Specify the base image you want to use for your application. This can be an official image from Docker Hub or a custom image.\nDefine dependencies and configurations: Install any dependencies or packages required by your application. Configure environment variables, working directory, ports, and other necessary settings.\nCopy application files: Copy your application code and files into the image using the COPY instruction.\nExecute commands: Use the RUN instruction to execute any commands needed during the image build process, such as installing dependencies, compiling code, or running scripts.\nExpose ports (optional): If your application requires specific ports to be accessible, use the EXPOSE instruction to specify them.\nDefine the default command: Use the CMD instruction to define the command that should be run when a container based on the image is started.\nBuild the image: Open a terminal or command prompt, navigate to the directory containing the Dockerfile, and run the docker build command, specifying a tag for the image. For example:\n\n\ndocker build -t myimage:latest .\n\nWait for the build process: Docker will execute the instructions in the Dockerfile and build the image. This may take some time, depending on the size of the image and the complexity of the instructions.\nVerify the image: After the build process completes successfully, you can verify the newly created image by running docker images and checking for the image with the specified tag."}},"/Docker/Docker-Architecture":{"title":"Docker Architecture","data":{"":"Docker uses a client-server architecture.\nThe Docker client interacts with the Docker daemon.\nThe Docker daemon performs tasks such as building, running, and distributing Docker containers.\nThe Docker client and daemon can be on the same system or connected remotely.\nCommunication between the Docker client and daemon occurs through a REST API.\nThe communication can take place over UNIX sockets or a network interface.\nDocker Compose is another client that allows working with applications consisting of multiple containers.\nDocker Compose helps manage the configuration and deployment of multi-container applications.","reference#Reference":"Docker Architecture -> https://docs.docker.com/get-started/overview/"}},"/Docker/Docker-Client":{"title":"Docker Client","data":{"":"It is the primary tool used to execute commands and manage Docker resources.\nUsers interact with the Docker client to perform actions such as running containers using the docker run command.\nThe Docker client sends commands to the Docker daemon (dockerd) for execution.\nIt utilizes the Docker API to communicate with the Docker daemon.\nThe Docker client can communicate with multiple Docker daemons if necessary.\nIt provides a command-line interface (CLI) for executing Docker commands and managing Docker objects.\nThe Docker client plays a crucial role in orchestrating Docker operations and managing the Docker environment."}},"/Docker/Docker-Demon":{"title":"Docker Demon","data":{"":"The Docker daemon, also known as dockerd, is a core component of Docker.\nIt serves as the main process running on a Docker host, responsible for managing Docker objects.\nThe Docker daemon listens for Docker API requests, allowing clients to interact with Docker.\nIt manages various Docker objects, including images, containers, networks, and volumes.\nThe Docker daemon handles tasks such as creating, starting, stopping, and deleting Docker containers.\nIt is responsible for managing the lifecycle of Docker services and coordinating their operations.\nThe Docker daemon can communicate with other daemons, enabling the management of Docker services across multiple hosts.\nDocker clients, such as the Docker command-line interface (CLI) and Docker Compose, interact with the Docker daemon to execute commands and manage Docker resources."}},"/Docker/Docker-Desktop":{"title":"Docker Desktop","data":{"":"Docker Desktop is an application designed for Mac, Windows, and Linux environments.\nIt provides an easy installation process to set up Docker on your machine.\nDocker Desktop enables the building and sharing of containerized applications and microservices.\nIt includes essential components such as the Docker daemon (dockerd) and the Docker client (docker).\nDocker Compose, Docker Content Trust, Kubernetes, and Credential Helper are also included in Docker Desktop.\nDocker Desktop offers a user-friendly interface and environment for managing Docker resources.\nIt simplifies the development and deployment of containerized applications on local machines.\nDocker Desktop is a comprehensive toolset that integrates multiple Docker-related functionalities.\nFor more detailed information about Docker Desktop, it is recommended to refer to the official Docker Desktop documentation.","reference#Reference":"Docker Desktop -> https://docs.docker.com/get-started/overview/"}},"/Docker/Docker-Image":{"title":"Docker Image","data":{"":"An image in Docker is a read-only template containing instructions for creating a Docker container.\nImages are often based on other images, allowing for customization and additional layers.\nFor example, an image can be based on the Ubuntu image and include the Apache web server, application, and necessary configurations.\nImages can be created by individuals or obtained from registries where they are published by others.\nTo build a custom image, a Dockerfile is used, which defines the steps to create and run the image.\nEach instruction in a Dockerfile creates a layer in the image, allowing for incremental and efficient updates.\nWhen changes are made to the Dockerfile and the image is rebuilt, only the affected layers are rebuilt, making images lightweight and fast.\nDocker images are designed to be smaller and more efficient compared to other virtualization technologies.\nImages serve as a basis for creating and running containers, providing a consistent environment for applications."}},"/Frontend Build Tools/Parcel":{"title":"Parcel","data":{"":"[Parcel]https://parceljs.org/"}},"/Docker/Docker File for Nextjs":{"title":"Docker File for Nextjs","data":{"":"# Use an official Node.js image as the base\nFROM node:14-alpine\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copy package.json and package-lock.json to the working directory\nCOPY package*.json ./\n\n# Install project dependencies\nRUN npm install\n\n# Copy the entire project to the working directory\nCOPY . .\n\n# Build the Next.js application\nRUN npm run build\n\n# Expose the desired port (change it to match your Next.js application's port)\nEXPOSE 3000\n\n# Set the command to start the Next.js application\nCMD [\"npm\", \"start\"]\nMake sure to replace 3000 with the actual port number your Next.js application listens on. This Dockerfile assumes that your project structure includes package.json, package-lock.json, and a build script defined in scripts section of package.json.To build a Docker image using this Dockerfile, navigate to the directory containing the Dockerfile and run the following command:\ndocker build -t your-image-name .\nReplace your-image-name with the desired name for your Docker image.Once the image is built, you can run a container based on this image using the following command:\ndocker run -p 3000:3000 your-image-name\nThis will map port 3000 of the container to port 3000 of your host machine. You can then access your Next.js application by visiting http://localhost:3000 in your browser.Remember to customize the Dockerfile as per your project's specific requirements, such as additional dependencies or environment variables."}},"/Frontend Extra Tools/Algolia":{"title":"Algolia","data":{"":"https://www.algolia.com/"}},"/Frontend Build Tools/webpack":{"title":"Webpack","data":{"":"webpackWebpack is a popular open-source JavaScript module bundler primarily used for web development. It provides a powerful and flexible solution for managing and bundling dependencies, assets, and resources in a web application.Key points about webpack:\nModule Bundler: Webpack takes modules with dependencies and generates optimized bundles for web applications. It allows developers to organize their codebase into small, manageable modules, making it easier to maintain and reuse code.\nDependency Resolution: Webpack analyzes the dependencies between modules and builds a dependency graph. This graph allows webpack to determine the order in which modules should be bundled and resolves any nested dependencies.\nAsset Transformation: Webpack can handle various types of assets, not just JavaScript files. It has built-in loaders that transform different types of files, such as CSS, images, fonts, and more. Loaders allow developers to apply transformations, preprocessors, or optimizations on the assets as they are bundled.\nCode Splitting: Webpack enables code splitting, which allows the separation of application code into smaller chunks. This helps optimize the initial loading time of the application by only loading the necessary code for a specific page or feature. It also facilitates lazy loading of modules, reducing the overall bundle size.\nDevelopment Server: Webpack provides a development server that allows developers to run and test their applications locally. It includes features like live reloading and Hot Module Replacement (HMR), which updates the modules in the browser without a full page refresh, making the development process faster and more efficient.\nConfiguration: Webpack uses a configuration file (typically named webpack.config.js) to define the desired behavior and settings for the bundling process. The configuration file specifies entry points, output paths, module rules, plugins, and other options to customize how webpack operates.\nPlugin System: Webpack has a rich ecosystem of plugins that extend its functionality. Plugins can perform a wide range of tasks, such as code optimization, asset management, environment variables injection, and integration with other tools or frameworks.\nDevelopment and Production Modes: Webpack supports different modesâ€”development and production. In development mode, the emphasis is on providing useful development features like source maps and readable output. In production mode, the focus is on optimizing and minimizing the bundle size for better performance in a production environment.\nIntegration with Build Tools and Frameworks: Webpack can be seamlessly integrated into various build tools and frameworks, such as React, Angular, Vue.js, and many others. These frameworks often provide specific webpack configurations or plugins to streamline the development workflow."}},"/Frontend Build Tools/Vite":{"title":"Vite","data":{"":"ViteVite is a build tool and development server designed for modern web development. It focuses on providing a fast and optimized development experience by leveraging native ES modules in the browser and utilizing build tooling written in compile-to-native languages.Key points about Vite:\nNative [[ES modules]]: Vite takes advantage of the native ES modules support in modern browsers. Instead of bundling the code like traditional build tools, Vite serves the modules directly to the browser, leveraging their native loading capabilities. This eliminates the need for time-consuming bundling steps during development.\nFaster Development Server: Vite provides a fast development server that starts up quickly, reducing the waiting time for developers. It achieves this by serving individual modules on-demand, resulting in faster loading and updating of the application in the browser.\nInstant HMR (Hot Module Replacement): Vite supports Hot Module Replacement, allowing developers to see instant updates in the browser as they make changes to their code. With Vite, module-level changes can be reflected immediately without needing a full page reload, providing a smooth and efficient development experience.\nOptimized for Vue and React: Vite has built-in support and optimizations specifically tailored for Vue.js and React. It understands the underlying frameworks and applies optimizations like faster component hot-reloading and more efficient CSS handling, resulting in improved performance during development.\nDevelopment Environment Pre-configured: Vite comes with a pre-configured development environment, eliminating the need for extensive manual configuration. It sets up a sensible default configuration based on the chosen framework (Vue.js, React, or vanilla JavaScript) to enable a smooth development experience out of the box.\nPlugin Ecosystem: Vite has a plugin ecosystem that allows developers to extend its functionality. Plugins can be used to enhance the build process, add custom features, or integrate with other tools or frameworks. The plugin system enables developers to customize and tailor Vite to their specific project requirements.\nProduction-Ready Builds: Although Vite is primarily designed for development, it can also generate optimized production-ready builds. It utilizes a production build process that bundles and optimizes the application code for deployment, ensuring high performance in a production environment.\nLanguage Support: Vite supports various languages that compile to JavaScript, such as TypeScript, CoffeeScript, and JSX, providing a flexible development environment for developers who prefer different languages.\n\nVite's focus on leveraging native ES modules, providing a fast development server, and optimizing the development experience for specific frameworks like Vue.js and React makes it a compelling choice for modern web development. Its efficient module loading, instant HMR, and streamlined configuration make it suitable for building performant and responsive web applications."}},"/Frontend Concepts/ES modules":{"title":"Es Modules","data":{"":"Prior to the introduction of ES modules, developers relied on other mechanisms like CommonJS or AMD (Asynchronous Module Definition) to modularize their JavaScript code. These mechanisms required the use of additional tools and bundlers to combine and package the modules for the browser.\nHowever, starting with ECMAScript 2015 (ES6), the JavaScript language introduced native support for modules through the import and export syntax. This allowed developers to write JavaScript code in a more modular and organized manner directly in their files.\nUnfortunately, during the initial stages of ES module support, web browsers did not immediately provide native support for loading and executing ES modules. Instead, developers had to rely on bundlers like [[webpack]], [[Rollup]], or [[Parcel]] to transpile and bundle their ES module-based code into a format that browsers could understand.\nIn recent years, most modern browsers have started to support native ES modules, allowing developers to directly use the import and export syntax in their JavaScript code without the need for bundling or transpilation. This native support provides benefits such as faster loading times, better caching, and improved browser compatibility."}},"/Frontend Extra Tools/Eslint":{"title":"Eslint","data":{"":"https://eslint.org/"}},"/Git/Git Divergent Branch":{"title":"Git Divergent Branch","data":{"":"Divergent Git branches refer to branches that have diverged from each other. This means that the branches have separate and distinct commits that are not present in the other branch. In other words, both branches have progressed independently and have different sets of commits.When branches are divergent, it typically indicates that changes have been made in parallel on both branches, resulting in different commit histories. This can happen when multiple developers are working on the same project and making changes on different branches simultaneously.Divergent branches can occur in various scenarios, such as:\nFeature branches: Different developers may be working on different features or bug fixes in separate branches. Each branch will have its own set of commits specific to the respective changes.\nLong-lived branches: If a branch is not frequently updated or merged with other branches, it can become divergent over time as other branches progress with new commits.\n\nWhen you encounter divergent branches, Git needs instructions on how to reconcile the changes from both branches when performing operations like merging or pulling. You may need to choose the appropriate merge strategy ([[Git merge]], [[Git rebase]], or [[Git fast-forward]]) to bring the branches back together and combine the changes effectively.The commands you mentioned are related to configuring the behavior of the git pull command when there are divergent branches (branches that have diverged and have separate commits). Here's an explanation of each command:\ngit config pull.rebase false:\nThis command sets the configuration option pull.rebase to false.\nWhen you run git pull after setting this option, Git will perform a merge operation to reconcile the divergent branches.\nIn other words, it will create a new merge commit that combines the changes from both branches.\n\n\ngit config pull.rebase true:\nThis command sets the configuration option pull.rebase to true.\nWhen you run git pull after setting this option, Git will perform a rebase operation to reconcile the divergent branches.\nIn other words, it will apply your local commits on top of the updated remote branch, effectively replaying your commits on top of the latest commits.\n\n\ngit config pull.ff only:\nThis command sets the configuration option pull.ff to only.\nWhen you run git pull after setting this option, Git will only perform a fast-forward merge to reconcile the divergent branches, if possible.\nFast-forward merging occurs when the branch you are merging into is directly ahead of your current branch, so Git can simply move your branch pointer forward to match the updated branch pointer without creating a new merge commit.\n\n\n\nBy configuring these options, you can specify how Git should handle divergent branches when you run git pull. Each option provides a different approach to reconciling the branches: merge, rebase, or fast-forward only. Choose the option that aligns with your preferred workflow and the requirements of your project."}},"/Frontend Extra Tools/Prettier":{"title":"Prettier","data":{"":"https://prettier.io/"}},"/Git/Git Ignore":{"title":"Git Ignore","data":{"":"To use the Git ignore file, you can follow these steps:\nCreate a file named .gitignore in the root directory of your Git repository. The file should be named exactly .gitignore, without any file extension.\nOpen the .gitignore file in a text editor of your choice.\nAdd the file and directory patterns that you want Git to ignore. Each pattern should be on a separate line. For example:\n# Ignore build artifacts\nbuild/\n\n# Ignore log files\n*.log\n\n# Ignore configuration files\nconfig.ini\nIn the above example, build/ is a directory that Git will ignore, *.log represents any files ending with .log that Git will ignore, and config.ini is a specific file that Git will ignore.You can use various patterns, including wildcards (*), directories (/), and negations (!). Comments can also be added using the # symbol.\nSave the .gitignore file.\nCommit the .gitignore file to your Git repository. Use the following commands:\ngit add .gitignore\ngit commit -m \"Add .gitignore file\"\nThis adds the .gitignore file to the repository and records the commit.\n\nOnce the .gitignore file is committed, Git will automatically ignore the files and directories specified in the file. It will not track or consider them for any Git operations, such as git status, git add, or git commit.Note that the .gitignore file can be used to ignore specific files or directories within your Git repository. It is useful for excluding generated files, build artifacts, log files, editor-specific files, and other files that are not part of your project's source code."}},"/Git/Git Commands":{"title":"Git Commands","data":{"":"[[Git Show]]\n[[Git Previous Commits]]\n[[Git Ignore]]\n[[Git Remove Cache]]"}},"/Git/Git Previous Commits":{"title":"Git Previous Commits","data":{"":"To check how your Git directory looked like and what files were present in previous commits, you can use the git log command in combination with the --name-only option. Here's how you can do it:\nOpen your command line or terminal.\nNavigate to the root directory of your Git repository.\nRun the following command:\ngit log --name-only\nThis command will display a log of all the previous commits in your repository, along with the list of files changed in each commit.The output will include commit information, such as the commit hash, author, date, and commit message, followed by the list of files changed in that commit.If you only want to see the file names without the commit information, you can use the --name-only option:\ngit log --name-only --pretty=format:\nThe --pretty=format: option ensures that only the file names are displayed.Additionally, you can use other options with git log to customize the output further. For example, you can limit the log to a specific branch, author, or time range using the respective options (--branches, --author, --since, etc.).\n\nBy running git log with the --name-only option, you can get an overview of the files changed in each commit and understand how your Git directory looked like in the previous commits."}},"/Git/Git Show":{"title":"Git Show","data":{"":"To check the previous version of a specific file in your Git repository, you can use the git show command followed by the commit hash or branch name and the path to the file. Here's the syntax:\ngit show <commit>:<file-path>\nReplace <commit> with the commit hash or branch name that contains the desired version of the file, and <file-path> with the path to the file within the repository.For example, to check the previous version of the index.html file in the current branch, you can use the following command:\ngit show HEAD~1:index.html\nHere, HEAD~1 refers to the previous commit relative to the current commit, and index.html is the file you want to view.If you know the commit hash where the previous version of the file exists, you can use that instead of HEAD~1. For instance:\ngit show abc123:index.html\nReplace abc123 with the actual commit hash.Running the git show command will display the content of the file as it existed in the specified commit or branch. You will see the changes made in that particular version of the file along with other commit information.Note that git show only displays the content of a single file at a specific commit. If you want to compare multiple versions of a file or view more extensive history, you may consider using Git graphical tools or other commands like git log or git diff."}},"/Git/Git Remove Cache":{"title":"Git Remove Cache","data":{"":"If your Git push is trying to commit node_modules even though the node_modules folder is not present in your Git repository, it may be due to the presence of the node_modules entry in the Git cache. To fix this issue, you can follow these steps:\nRemove the node_modules entry from the Git cache using the following command:\ngit rm -r --cached node_modules\nThis command removes the node_modules entry from the Git cache without deleting the actual node_modules folder from your local file system.\nCreate or update your .gitignore file in the root directory of your Git repository to ensure that node_modules is ignored. Add the following line to your .gitignore file:\nnode_modules/\nThis will prevent Git from tracking and committing the node_modules folder in the future.\nCommit the changes to your repository, including the removal of the node_modules entry from the cache and the updated .gitignore file:\ngit add .\ngit commit -m \"Remove node_modules from Git cache and update .gitignore\"\n\nNow, when you push your changes, Git should no longer attempt to commit the node_modules folder.\n\nIt's important to note that the above steps will only prevent the node_modules folder from being committed in future commits. If the node_modules folder was previously committed, it will still be present in the Git history. To completely remove node_modules from your Git history, you would need to perform a more advanced operation like Git history rewriting, such as using the git filter-branch command. However, be cautious when rewriting Git history, as it can have implications for you and other collaborators working on the repository."}},"/Git/Git fast-forward":{"title":"Git Fast Forward","data":{"":"Fast-forward is a merging scenario where Git can directly move the branch pointer forward to incorporate the changes, without creating a new merge commit.\nIt occurs when the branch being merged into is ahead of the branch being merged.\nHere's how a fast-forward merge works:\nSwitch to the branch that you want to merge into (git checkout <branch>).\nRun the merge command (git merge <source-branch>), where <source-branch> is the branch you want to merge into the current branch.\nIf the branch being merged into is directly ahead of the source branch, Git will perform a fast-forward merge by moving the branch pointer forward to match the source branch's latest commit."}},"/Git/Git rebase":{"title":"Git Rebase","data":{"":"Rebase is a Git operation that moves or reapplies commits from one branch onto another branch. It essentially modifies the commit history of the branch being rebased.\nIt's useful when you want to incorporate the latest changes from a source branch onto a target branch, making the commit history linear and cleaner.\nHere's how you can perform a rebase:\nSwitch to the branch where you want to apply the changes (git checkout <target-branch>).\nRun the rebase command (git rebase <source-branch>), where <source-branch> is the branch containing the latest changes you want to apply.\nGit will calculate the changes introduced in the source branch and apply them on top of the target branch.\nIf there are any conflicts, you'll need to manually resolve them as Git applies each commit.\nAfter resolving conflicts (if any), continue the rebase process (git rebase --continue) until all commits are applied.\nOnce the rebase is complete, you'll have a linear commit history on the target branch."}},"/Git/Git merge":{"title":"Git Merge","data":{"":"Merge is a Git operation that combines the changes from different branches into one branch, creating a new commit that represents the merge result.\nIt's commonly used when you want to integrate changes from one branch into another branch.\nHere's how you can perform a merge:\nSwitch to the branch where you want to merge the changes (git checkout <branch>).\nRun the merge command (git merge <source-branch>), where <source-branch> is the branch from which you want to merge the changes.\nGit will attempt to automatically merge the changes. If there are any conflicts, you'll need to manually resolve them.\nAfter resolving conflicts (if any), commit the merge changes (git commit) with an appropriate commit message."}},"/Markdown/Markdown":{"title":"Markdown","data":{"":"Lightweight Markup Language: Markdown is a lightweight markup language that uses plain text formatting to create structured documents without the need for complex HTML or formatting syntax.\nEasy to Read and Write: Markdown files are designed to be easy to read and write, with a syntax that is simple and intuitive.\nCross-Platform Compatibility: Markdown files can be created and viewed on any platform using a simple text editor. They can also be converted to various other file formats, such as HTML or PDF, with the help of tools or converters.\nText-Based Formatting: Markdown allows you to format text using simple symbols and characters, such as asterisks (*), underscores (_), and hash symbols (#), to indicate headings, emphasis, lists, links, and more.\nPlain Text Structure: Markdown files maintain a clear and readable structure even in their raw form, making them easy to track and version control using tools like Git.\nWidely Supported: Markdown is widely supported across various platforms, applications, and content management systems (CMS). It is commonly used for writing documentation, creating web content, and even in note-taking applications.\nExtensible: Markdown supports extensions or flavors that add additional functionality and features. Common examples include GitHub Flavored Markdown (GFM) and Markdown Extra, which provide enhanced syntax for tasks like tables, task lists, code highlighting, and more.\nIntegration with HTML: Markdown allows you to embed HTML code within the document when you need more advanced formatting or customization.\nReadable Output: When converted to HTML or other formats, Markdown files produce clean, well-structured, and readable output that can be easily rendered in web browsers or other applications.\nPortable and Future-Proof: Markdown files are portable and future-proof because they rely on plain text and have a simple syntax that is likely to remain compatible with future software and tools.\n\n[[MDX]] is advanced version of markdown where you can use react components to render inside a .md file.\n[[Mermaid]]"}},"/Markdown/MDX":{"title":"Mdx","data":{"":"Certainly! Here are some key important points about MDX (Markdown + JSX) files:\nCombination of Markdown and JSX: MDX is an extension of Markdown that allows you to embed JSX (JavaScript XML) components and code directly within Markdown files.\nEnhanced Component Support: MDX enables the use of React components within Markdown, allowing for more interactive and dynamic content creation.\nFlexible and Powerful: MDX provides the flexibility of Markdown's simplicity and readability, combined with the power and extensibility of React components.\nSeamless Integration: MDX files can seamlessly integrate with existing React and JavaScript ecosystems, making it easy to combine static content with interactive elements and functionality.\nInteractive Documentation: MDX is commonly used for creating interactive documentation, tutorials, and blog posts, where rich content and code examples can be seamlessly combined.\nCode Highlighting: MDX supports syntax highlighting for code snippets, making it easier to showcase and explain code examples within the document.\nCustomizable and Extensible: MDX allows for the creation of custom components and the use of third-party libraries, enabling developers to tailor the document's functionality to their specific needs.\nAdvanced Content Creation: With MDX, you can include complex interactive elements, such as charts, diagrams, videos, and more, to enhance the content and engage readers.\nAccess to JavaScript Ecosystem: MDX files have access to the entire JavaScript ecosystem, including npm packages, utilities, and frameworks, providing developers with a wide range of resources to enhance their content.\nRendered Output: MDX files can be transformed into static HTML or JSX components, allowing them to be rendered in web browsers or integrated into React applications."}},"/Markdown/Mermaid":{"title":"Mermaid","data":{"":"https://mermaid.js.org/intro/"}},"/Nextjs/Nextjs vs Remix":{"title":"Nextjs Vs Remix","data":{"":"https://www.altogic.com/blog/nextjs-vs-remix"}},"/Nextjs/Nextjs to ghpages":{"title":"Nextjs to Ghpages","data":{"":""}},"/Nextjs/Nextjs":{"title":"Nextjs","data":{"":"https://nextjs.org/"}},"/Nextjs/Remix":{"title":"Remix","data":{"":"https://remix.run/"}},"/Obsidian/Zettelkasten Approach":{"title":"Zettelkasten Approach","data":{"":"Zettelkasten Approach of note taking.\n\n#Obsidian"}},"/Nextjs/Nextra":{"title":"Nextra","data":{"":"shttps://nextra.site/"}},"/React/Index":{"title":"Index","data":{"":"","react#React":"[[Introduction]]\n[[React Basics]]\n[[React Api Referece]]\n[[React News]]\n[[React Developer Tools]]\n\nVite ->https://vitejs.dev/\nParcel -> https://parceljs.org/\nEslint -> https://eslint.org/\nPrettier -> https://prettier.io/","nextjs-vs-remix---httpswwwaltogiccomblognextjs-vs-remix#Nextjs vs Remix -> https://www.altogic.com/blog/nextjs-vs-remix":"Remix -> https://remix.run/\nReact Native\nIntroduction -> https://reactnative.dev/\n\nAlgolia\nAI Search solutions -> https://www.algolia.com/"}},"/React/Introduction":{"title":"Introduction","data":{"":"Reference -> https://react.dev/"}},"/Obsidian/Organizing notes in Obsidian":{"title":"Organizing Notes in Obsidian","data":{"":"I prefer using tags and nested tags to organizing obsidian notes.\n[[Organizing notes in Obsidian]].\n\n#Obsidian"}},"/React/React Basics":{"title":"React Basics","data":{"":"https://react.dev/learn/react-developer-tools"}},"/Obsidian/Obsidian":{"title":"Obsidian","data":{"":"Obsidian is a note-taking app known for its flexibility and interconnectedness.\nIt uses a plain-text format [[Markdown]] for future-proofing and accessibility.\nFeatures bidirectional linking to create connections between notes.\nOffers a graph view for visual representation of note relationships.\nSupports collaboration through syncing with cloud storage services.\nHighly customizable with personalization options and community-developed plugins.\nEmphasizes organization and exploration of knowledge.\n\nReference ->\n[[Zettelkasten Approach]]\n[[Organizing notes in Obsidian]]Tags ->\n#Obsidian"}},"/React/React Api Referece":{"title":"React API Referece","data":{"":"https://react.dev/learn/react-developer-tools"}},"/React/React News":{"title":"React News","data":{"":"https://react.dev/blog"}},"/React/React Native":{"title":"React Native","data":{"":"https://reactnative.dev/"}},"/React/React Developer Tools":{"title":"React Developer Tools","data":{"":"https://react.dev/learn/react-developer-tools"}},"/React/React":{"title":"React","data":{"":"[[Introduction]]\n[[React Basics]]\n[[React Api Referece]]\n[[React News]]\n[[React Developer Tools]]\n\nVite ->https://vitejs.dev/\nParcel -> https://parceljs.org/\nEslint -> https://eslint.org/\nPrettier -> https://prettier.io/","nextjs-vs-remix---httpswwwaltogiccomblognextjs-vs-remix#Nextjs vs Remix -> https://www.altogic.com/blog/nextjs-vs-remix":"Remix -> https://remix.run/\nReact Native\nIntroduction -> https://reactnative.dev/\n\nAlgolia\nAI Search solutions -> https://www.algolia.com/"}},"/Vercel/Vercel Introuction":{"title":"Vercel Introuction","data":{"":"Vercel is a cloud platform that provides a serverless deployment infrastructure for web applications. It allows developers to build, deploy, and scale websites and web applications with ease. Key points about Vercel include:\nVercel focuses on simplifying the deployment process for front-end applications and static websites.\nIt supports various popular frameworks and libraries like [[Next.js]], [[React]], [[Angular]], and [[Vue.js]].\nOffers features such as automatic deployment, serverless functions, and global CDN (Content Delivery Network) for optimized performance.\nProvides a seamless integration with version control systems like GitHub and GitLab, enabling easy deployment directly from repositories.\nOffers advanced features like preview deployments, environment variables management, and custom domains.\nVercel provides a user-friendly interface and a command-line interface (CLI) for managing deployments and configuring project settings.\nIt is a popular choice for Jamstack (JavaScript, APIs, and Markup) projects and enables fast and scalable deployments for modern web applications.\n\nIn summary, Vercel is a cloud platform that simplifies the deployment process for front-end applications and static websites, offering features like automatic deployment, serverless functions, and global CDN. It is widely used for Jamstack projects and provides a seamless integration with version control systems.\n[[Vite]]\n[[Parcel]]\n[[Eslint]]\n[[Prettier]]\n[[Nextjs vs Remix]]\n[[Remix]]\n[[React Native]]\n[[Algolia]]\n[[Nextjs]]\n[[Nextra]]\n[[Deploy Nextjs to Vercel]]#Frontend\n#Vercel"}},"/Vercel/Deploy Nextjs to Vercel":{"title":"Deploy Nextjs to Vercel","data":{"":"Login to Vercel.\nImport Get repository which needs to be deployed.\nGrant necessary permissions to vercel.\nClick on Import Button.\nClick on Deploy button\nYou could use a Template provided by Vercel of would have created frontend project from scratch"}},"/Vocabulary/Vocabulary":{"title":"Vocabulary","data":{"":"Non-Trivial ->\nThe term \"non-trivial\" is often used to describe something that is not obvious, simple, or easy to solve. It refers to a problem, task, or concept that requires significant effort, creativity, or expertise to understand or address.\nCurated ->\nThe term \"curated\" is commonly used to describe the act of carefully selecting, organizing, and presenting a collection of items or information in a thoughtful and deliberate manner. It involves the process of gathering, evaluating, and choosing items based on their relevance, quality, and significance to create a curated collection or experience.\nMyriad ->\nThe term \"myriad\" is an adjective that means a countless or extremely large number of something. It is often used to describe a vast and indefinite quantity or variety of things. The word \"myriad\" can also be used as a noun to refer to a great number or multitude of things.\nDemocratized ->\nThe term \"democratized\" refers to the process of making something accessible, available, or participatory to a broader range of people, often with the aim of promoting equality, inclusivity, and widespread participation. It involves removing barriers, restrictions, or exclusivity that may have previously limited access or participation in a particular area or domain.\nAmassed ->\nThe term \"amassed\" refers to the action of collecting, gathering, or accumulating a large quantity or number of something over time. It implies the process of gradually acquiring or bringing together items, resources, wealth, or information to form a significant or substantial whole.\n\n#Vocabulary"}},"/Work in Progress/Observibility":{"title":"Observibility","data":{"":"Create a Dockerfile: Start by creating a text file called \"Dockerfile\" (with no file extension) in the root directory of your project.\nChoose a base image: Specify the base image you want to use for your application. This can be an official image from Docker Hub or a custom image.\nDefine dependencies and configurations: Install any dependencies or packages required by your application. Configure environment variables, working directory, ports, and other necessary settings.\nCopy application files: Copy your application code and files into the image using the COPY instruction.\nExecute commands: Use the RUN instruction to execute any commands needed during the image build process, such as installing dependencies, compiling code, or running scripts.\nExpose ports (optional): If your application requires specific ports to be accessible, use the EXPOSE instruction to specify them.\nDefine the default command: Use the CMD instruction to define the command that should be run when a container based on the image is started.\nBuild the image: Open a terminal or command prompt, navigate to the directory containing the Dockerfile, and run the docker build command, specifying a tag for the image. For example:\n\n\ndocker build -t myimage:latest .\n\nWait for the build process: Docker will execute the instructions in the Dockerfile and build the image. This may take some time, depending on the size of the image and the complexity of the instructions.\nVerify the image: After the build process completes successfully, you can verify the newly created image by running docker images and checking for the image with the specified tag."}},"/about":{"title":"DevOps Applications Automation Team Lead","data":{"":"Welcome to my portfolio site! I am a DevOps Applications Automation Team Lead with a passion for building efficient and scalable software solutions. Here's some information about me:\nðŸ”­ I'm currently working on My DevOps Large Enterprise Practical Guide, where I share my knowledge and insights on DevOps practices for large enterprise environments.\nðŸŒ± I'm currently learning CI/CD in Test Automation Frameworks to enhance my skills in building robust and automated software delivery pipelines.\nðŸ‘¯ I'm looking to collaborate on DevOps projects and work with like-minded professionals to drive innovation and improve software development processes.\nðŸ¤” I'm looking for help with open source DevOps projects where I can contribute my expertise and learn from the community.\nðŸ’¬ Feel free to ask me about anything related to DevOps. I'm always happy to share my knowledge and engage in meaningful discussions.\nðŸ“« You can reach me via email at sunnyravbusiness@gmail.com. I'm open to networking, collaboration, and discussing new opportunities.\nâš¡ Fun fact: I enjoy doing squats as a form of exercise to stay active and maintain a healthy lifestyle.","skills#Skills":"Here are some of my key skills and proficiencies:\nDevOps: â­â­â­â­â­â­â­â­â­ (9/10)\nDocker: â­â­â­â­â­â­â­â­â­ (9/10)\nAWS Pipelines: â­â­â­â­â­â­â­â­ (8/10)\nAWS: â­â­â­â­â­â­â­ (7/10)\nJava: â­â­â­â­â­â­ (6/10)\nSelenium: â­â­â­â­â­ (5/10)\nThese skills reflect my experience and proficiency in various aspects of DevOps, containerization with Docker, AWS pipelines, AWS services, Java programming, and test automation with Selenium.\nCertificationI hold the AWS Solutions Architect Associate certification, which validates my knowledge and expertise in architecting solutions on the Amazon Web Services (AWS) platform. This certification demonstrates my ability to design and deploy scalable, reliable, and cost-effective applications on AWS.\nVisit My SiteFor more information about me, my projects, and my experience, please visit my portfolio site (opens in a new tab). The site provides additional details and showcases my work in the field of DevOps and software automation.Thank you for visiting my portfolio. I look forward to connecting with you and discussing exciting opportunities in the world of DevOps!"}},"/Work in Progress/Elastic Search":{"title":"Elastic Search","data":{"":"https://www.elastic.co/what-is/elasticsearch"}},"/":{"title":"DevOps Applications Automation Team Lead","data":{"":"Welcome to my portfolio site! I am a DevOps Applications Automation Team Lead with a passion for building efficient and scalable software solutions. Here's some information about me:\nðŸ”­ I'm currently working on My DevOps Large Enterprise Practical Guide, where I share my knowledge and insights on DevOps practices for large enterprise environments.\nðŸŒ± I'm currently learning CI/CD in Test Automation Frameworks to enhance my skills in building robust and automated software delivery pipelines.\nðŸ‘¯ I'm looking to collaborate on DevOps projects and work with like-minded professionals to drive innovation and improve software development processes.\nðŸ¤” I'm looking for help with open source DevOps projects where I can contribute my expertise and learn from the community.\nðŸ’¬ Feel free to ask me about anything related to DevOps. I'm always happy to share my knowledge and engage in meaningful discussions.\nðŸ“« You can reach me via email at sunnyravbusiness@gmail.com. I'm open to networking, collaboration, and discussing new opportunities.\nâš¡ Fun fact: I enjoy doing squats as a form of exercise to stay active and maintain a healthy lifestyle.","skills#Skills":"Here are some of my key skills and proficiencies:\nDevOps: â­â­â­â­â­â­â­â­â­ (9/10)\nDocker: â­â­â­â­â­â­â­â­â­ (9/10)\nAWS Pipelines: â­â­â­â­â­â­â­â­ (8/10)\nAWS: â­â­â­â­â­â­â­ (7/10)\nJava: â­â­â­â­â­â­ (6/10)\nSelenium: â­â­â­â­â­ (5/10)\n\nThese skills reflect my experience and proficiency in various aspects of DevOps, containerization with Docker, AWS pipelines, AWS services, Java programming, and test automation with Selenium.","certification#Certification":"I hold the AWS Solutions Architect Associate certification, which validates my knowledge and expertise in architecting solutions on the Amazon Web Services (AWS) platform. This certification demonstrates my ability to design and deploy scalable, reliable, and cost-effective applications on AWS.","visit-my-site#Visit My Site":"For more information about me, my projects, and my experience, please visit my portfolio site. The site provides additional details and showcases my work in the field of DevOps and software automation.Thank you for visiting my portfolio. I look forward to connecting with you and discussing exciting opportunities in the world of DevOps!"}},"/Docker/Docker Introduction":{"title":"Docker Introduction","data":{"":"Containers package code and its dependencies for reliable and fast execution.\nDocker containers are lightweight and standalone.\nThey include everything necessary to run an application: code, runtime, tools, libraries, and settings.\nContainer images become containers during runtime.\nDocker containers rely on Docker Engine to become containers.\nContainers work consistently across different infrastructures.\nContainers isolate software from the environment.\nContainers ensure uniform functionality across different stages like development and staging.","reference#Reference":"Documents -> https://www.docker.com\nGetting Started -> https://docs.docker.com/get-started/\nDownload -> https://docs.docker.com/get-docker/"}},"/Docker/Docker Basics":{"title":"Docker Basics","data":{"":"[[Docker Introduction]][[Docker-Architecture]][[Docker-Demon]][[Docker-Client]][[Docker-Desktop]][[Docker-Image]][[Docker-Container]][[Build-Container-Image]]","build-container-image-example#Build-Container-Image-Example":"# Use an official Python runtime as the base image\nFROM python:3.9-slim-buster\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the dependencies file to the working directory\nCOPY requirements.txt .\n\n# Install the dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the application code to the working directory\nCOPY . .\n\n# Expose a port (optional)\nEXPOSE 8000\n\n# Define the command to run the application\nCMD [\"python\", \"app.py\"]\n\nabove example uses the official Python 3.9 slim-buster image as the base.\nSets the working directory inside the container to /app.\nCopies the requirements.txt file from the host to the working directory in the container.\nInstalls the dependencies listed in requirements.txt using pip.\nCopies the entire application code from the host to the working directory in the container.\nOptionally exposes port 8000 to allow incoming connections (adjust the port number as needed).\nSets the command to run the application using the CMD instruction, which in this case runs the app.py script with Python.","start-container-image#Start-Container-Image":"Use the docker run command to start a container based on the image. Specify any desired options or configurations and provide the image name and tag. For example:\n\n\ndocker run -p 8000:8000 myimage:latest\nIn this example, -p 8000:8000 maps port 8000 on the host to port 8000 in the container. Adjust the port mapping as needed for your application.\n2. Wait for Docker to download the necessary layers and start the container. You should see the container ID or name printed in the terminal, indicating that the container is running.\n3. Access your application: If your application is a web server listening on port 8000, you can open a web browser and navigate to http://localhost:8000 to access it. Adjust the URL and port number based on your application's configuration.\n5. To stop the container, press Ctrl + C in the terminal where it is running. The container will be stopped and removed.","updating-docker-container#Updating-Docker-Container":"Make the necessary changes to your application source code.\nOpen a terminal or command prompt.\nBuild a new Docker image with the updated source code using the same Dockerfile and a new tag. Navigate to the directory containing the Dockerfile and run the docker build command again. For example:\n\n\ndocker build -t myimage:latest .\nMake sure to specify a different tag (e.g., myimage:updated) to differentiate it from the previous image.\n4. Wait for the new image to build. Docker will only rebuild the layers that have changed, making the process faster.\n5. Once the new image is built, stop and remove the existing container. Run the following command to list all running containers:\ndocker ps\nIdentify the container ID or name corresponding to the running container based on the previous image.\nThen, stop and remove the container using the docker stop and docker rm commands, respectively. For example:\ndocker stop container_id_or_name\ndocker rm container_id_or_name\n\nRun a new container using the updated image. Use the docker run command as mentioned earlier, specifying the updated image name and any desired options or configurations. For example:\n\n\ndocker run -p 8000:8000 myimage:updated\n\nWait for Docker to download the necessary layers and start the new container.\nAccess your application to verify that the changes have taken effect. Use the appropriate URL and port number as configured in your application.","docker-image-sharing#Docker-Image-Sharing":"Ensure that your containerized application is running and working correctly on your local machine. Verify that it functions as expected and that any necessary dependencies or configurations are in place.\nCommit the running container to a new Docker image. Open a new terminal or command prompt and use the docker commit command to create an image based on the running container. Specify the container ID or name and provide a name and tag for the new image. For example:\n\n\ndocker commit container_id_or_name myimage:shared\nReplace container_id_or_name with the actual ID or name of the running container, and choose a name and tag for the new image (e.g., myimage:shared).\n3. Tag the image with a version or any additional information if desired. This step is optional but can be helpful for tracking different versions of the shared image. For example:\ndocker tag myimage:shared myimage:v1.0\n\nLog in to a Docker registry (such as Docker Hub) if you haven't already. This step is necessary if you want to push the image to a registry that requires authentication. Use the docker login command and provide your credentials as prompted.\nPush the image to the Docker registry. Use the docker push command and specify the image name and tag to push it to the registry. For example:\n\n\ndocker push myimage:shared\n\nProvide the necessary information to others who wish to use your shared image. Share the image name and tag, along with any relevant instructions or documentation, so that others can pull and run the containerized application.\nOthers can now pull and run the shared image on their own machines using the docker pull and docker run commands, as described earlier. They should be able to use the image you shared to run the containerized application locally.","docker-registry#Docker-Registry":"A Docker registry is a storage system that stores Docker images.\nDocker Hub is a popular public registry available for general use.\nDocker is configured to search for images on Docker Hub by default.\nUsers can also set up their own private registry.\nThe docker pull or docker run commands retrieve required images from the configured registry.\nThe docker push command allows users to upload their own images to the configured registry.\nDocker registries enable efficient storage and distribution of Docker images.\nThey provide a centralized location for sharing and accessing container images.\nDocker Hub is a widely used public registry for discovering and sharing Docker images.\nPrivate registries offer additional control and privacy for organizations and individuals.\nA Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry.\nWhen you use the docker pull or docker run commands, the required images are pulled from your configured registry. When you use the docker push command, your image is pushed to your configured registry.\nSet up a Docker registry: You have the option to use Docker Hub (a public registry) or set up your own private registry. If you choose to set up a private registry, you can use tools like Docker Registry or third-party solutions.\nLog in to the Docker registry: If you are using Docker Hub or a private registry that requires authentication, log in to the registry using the docker login command. Provide your registry credentials (username and password) as prompted.\nTag your Docker image: Before pushing the image to the registry, it's recommended to tag it with the appropriate registry URL and repository name. Use the docker tag command to create a new tag for the image. For example:\n\n\ndocker tag myimage:latest myregistry/myrepository:latest\nReplace myimage:latest with the name and tag of the image you want to share. Adjust myregistry and myrepository with the appropriate registry URL and repository name.\n16. Push the Docker image to the registry: Use the docker push command to push the tagged image to the registry. For example:\ndocker push myregistry/myrepository:latest\nDocker will upload the image and its layers to the registry. This may take some time depending on the size of the image and your network connection.\n17. Verify the image on the registry: You can check the Docker registry (either Docker Hub or your private registry) to ensure that the image has been successfully pushed and is available for others to pull.\nDocker Registry -> https://docs.docker.com/get-started/overview/","docker-volumes#Docker-Volumes":"Docker volumes are managed by Docker and are independent of the host file system.\nVolumes are designed to persist data even if the container is stopped or deleted.\nVolumes can be created and managed using Docker commands or Docker Compose files.\nThey provide an easy way to share data between containers.\nDocker volumes can be named, making it convenient to reference them in multiple containers.\nVolumes can be created with specific drivers to support network storage, cloud providers, or other specialized storage systems.\nThe data inside a volume can be accessed by multiple containers simultaneously.\nVolumes have their own location on the host system, typically in the Docker directory.\n\n\ndocker run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v mysqlvolume:/var/lib/mysql mysql","docker-bind-mounts#Docker-Bind-Mounts":"Bind mounts are linked to a specific directory or file on the host system.\nWith bind mounts, you can directly reference files or directories on the host during container runtime.\nBind mounts do not have a separate life cycle from the host system; they are tightly coupled.\nChanges made in the bind mount are immediately visible on the host and vice versa.\nBind mounts provide a way to share files or directories between the host and containers.\nThey are useful for development workflows, where code changes can be immediately reflected inside the container.\nMultiple containers can bind to the same host directory or file, allowing easy data sharing.\nBind mounts can be specified using absolute paths or relative paths to the container's file system.","differences-between-bind-mount-and-volumes#Differences-Between-Bind-Mount-and-Volumes":"Persistence: Docker volumes persist data even if the container is removed, while bind mounts are tightly coupled to the host system and do not persist data separately.\nLifecycle: Docker volumes have a separate lifecycle managed by Docker, whereas bind mounts are directly linked to the host system and have no separate lifecycle.\nPortability: Docker volumes are portable and can be easily moved between different Docker hosts. Bind mounts are specific to the host system where the container is running.\nEase of use: Docker volumes provide a higher level of abstraction and ease of use, especially when managing data across multiple containers. Bind mounts are simpler to set up and are useful for immediate file or directory sharing.\nIntegration: Docker volumes can integrate with different storage drivers and systems, such as network storage or cloud providers. Bind mounts are limited to the host's file system.\nCertainly! Here are the point-wise details and differentiation for Docker container networks:","container-networks#Container-Networks":"Default Network: When Docker is installed, it automatically creates a default network called \"bridge.\" This network allows containers to communicate with each other using IP addresses.\nContainer Network Drivers: Docker supports multiple container network drivers that offer different networking capabilities, such as bridge, host, overlay, macvlan, and more. Each driver provides specific features and functionalities for container networking.\nBridge Network: The bridge network driver is the default and most commonly used driver. It creates an isolated network on the host and assigns IP addresses to containers connected to the bridge. Containers within the same bridge network can communicate with each other using container names as DNS aliases.\nHost Network: The host network driver allows a container to use the host's networking stack directly, sharing the same network interface. It gives the container access to all host interfaces, bypassing Docker's network isolation.\nOverlay Network: The overlay network driver enables containers to communicate across multiple Docker hosts or nodes in a swarm. It uses VXLAN (Virtual Extensible LAN) encapsulation to create a distributed network overlay that spans multiple hosts.\nMacvlan Network: The macvlan network driver assigns a unique MAC address to each container, allowing them to appear as individual devices on the physical network. Containers in a macvlan network can communicate with other devices on the same physical network, including other containers.\nNetwork Scopes: Docker provides different network scopes for containers, such as global, swarm, and local. Global scope allows containers to communicate across multiple Docker daemons. Swarm scope restricts communication to containers within the same swarm. Local scope confines communication to containers within the same host.\nUser-defined Networks: Docker allows users to create their own custom networks with specific configurations using the \"docker network create\" command. Custom networks provide better control over container connectivity and allow containers to be isolated in separate networks.\nNetwork Security: Docker networks can be secured using features like network segmentation, access control lists (ACLs), and firewall rules. This helps control traffic flow between containers and enhances network security.\nThird-party Networking Solutions: Docker integrates with third-party networking solutions, such as Calico, Weave, and Flannel, which provide advanced networking features like network policies, service discovery, and load balancing.","relationship-between-docker-desktop-and-docker-engine#Relationship-between-Docker-Desktop-and-Docker-Engine":"Docker Desktop includes Docker Engine as its core component. It installs and manages Docker Engine behind the scenes, providing a user-friendly interface to work with Docker containers.\nDocker Desktop abstracts away some of the complexities of working directly with Docker Engine, making it easier for developers to get started with Docker.\nDocker Desktop also includes additional tools and features specifically tailored for developers, such as the ability to configure container resources, networking, and storage options through its graphical interface.\nDocker Desktop is essentially a pre-packaged solution that bundles Docker Engine with a user-friendly interface and additional developer-centric features.","dns#DNS":"The docker containers talks to each other on the basis of DNS name(Docker container name) because the IP address from the container may be removed and can be dynamic at times(only apples to the new Virtual network)\nSo the default Bridge network does not have the same DNS functionality in it.(so you will have to add it manually by --link)","single-docker-example#Single-Docker-Example":"Spin up Nginx Server","reference#Reference":"Cloud-DevOps-Large-Enterprise-Practical-Guide/Docker","multiple-docker-example#Multiple-Docker-Example":"Spin up Nginx Server\nSpin up mysql server\nSpin up httpd Server","reference-1#Reference":"Cloud-DevOps-Large-Enterprise-Practical-Guide/Docker","docker-commands#Docker-Commands":"docker ->returns all the docker commands that can be used and referred to.\ndocker container run --publish 80:80 --detach --name mynewcontainer --network mynetwork nginx -> spins up a container spiting out a container ID\ndocker container stop (container name)-> stops the running container on the machine\ndocker container logs mynewcontainer\ndocker container top mynewcontainer\ndocker container rm (name of the containers separated by space)\nps aux (returns all the process running on the mac system)\ndocker container inspect mynewcontainer\ndocker container stats\ndocker container run -it --name enakonda nginx bash\ndocker container start -ai enakonda(This command keeps the docker container running until you have been exited from the bash shell)\ndocker container exec -it enakonda bash(This command keeps the docker container running)\ndocker container port enakonda\ndocker container inspect --format '{{ .NetworkSettings.IPAddress}}' enakonda.\ndocker container run -d --name enakonda --network-alias search elasticsearch:2\ndocker network ls\ndocker network inspect bridge\ndocker network create myNetwork\ndocker network connect network container\ndocker network disconnect network container\nImages are just the binaries and the Kernal is provided by the OS itself\ndocker image history\ndocker image tag oldimage new/imagename\ndocker image push\ndocker login\ndocker logout","reference-2#Reference":"Cloud-DevOps-Large-Enterprise-Practical-Guide/Docker","dns-round-robin#DNS-Round-Robin":"","env-variables#Env-Variables":"ENV KEY VALUE\n\n--e KEY=SomeOtherValue","arguments#Arguments":"ARG DEFAULT_PORT=80\n\n--build-arg DEFAULT_PORT=8080","markdown#Markdown":"GitHub Markdown is a lightweight markup language used to format and style text on GitHub.\nIt is based on the original Markdown syntax but includes some additional features and extensions.\nGitHub Markdown is commonly used in README files, documentation, and issue comments on GitHub.\nIt supports basic formatting options like headings, lists, emphasis (bold and italic), and links.\nGitHub Markdown allows for the inclusion of images, code blocks, and tables.\nIt supports syntax highlighting for different programming languages in code blocks.\nGitHub Markdown supports task lists, which are useful for tracking progress in issues or pull requests.\nIt allows for the creation of links to specific lines or sections within a file.\nGitHub Markdown supports emoji shortcuts for adding emoticons to your text.\nIt also supports the use of HTML tags for advanced formatting when necessary.","reference-3#Reference":"Documents -> https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/quickstart-for-writing-on-github","docker-swarm#Docker-Swarm":"","swarm-basics#Swarm-Basics":"Docker Swarm: Docker Swarm is a native clustering and orchestration solution for Docker. It allows you to create and manage a cluster of Docker nodes, turning them into a single virtual Docker engine.\nClustering: Docker Swarm enables you to create a cluster by joining multiple Docker hosts together. These hosts can be physical or virtual machines. The cluster behaves as a single entity, providing a unified interface for managing containers across all the nodes.\nOrchestration: Docker Swarm provides orchestration capabilities, allowing you to deploy and manage applications across the cluster. It handles tasks such as container scheduling, load balancing, service discovery, and scaling.\nNodes: In Docker Swarm, a node refers to an individual Docker host within the cluster. Nodes can be worker nodes or manager nodes. Worker nodes run containerized applications, while manager nodes handle cluster management tasks.\nServices: Services are the definition of the tasks you want to run in your cluster. You can define a service to run a single container or a group of containers, specifying details such as the image, ports, environment variables, and resource constraints.\nLoad Balancing: Docker Swarm automatically distributes containers across the available nodes to balance the workload. It uses an ingress load balancer to route incoming traffic to the appropriate container, ensuring high availability and scalability.\nScaling: With Docker Swarm, you can easily scale your services up or down to meet the demand. You can specify the desired number of replicas for a service, and Swarm will distribute them across the nodes accordingly.\nRolling Updates: Docker Swarm supports rolling updates, allowing you to update your services without causing downtime. It follows a rolling deployment strategy by gradually updating containers one by one, ensuring that the service remains available throughout the process.\nHigh Availability: Docker Swarm provides high availability by allowing you to configure manager node replicas. If one manager node fails, another takes over, ensuring the cluster remains operational.\nIntegration with Docker CLI: Docker Swarm integrates seamlessly with the Docker command-line interface (CLI), making it easy to work with existing Docker commands and tools.","initialization-and-management#Initialization and Management:":"docker swarm init: Initialize a new Docker Swarm on the current node.\ndocker swarm join: Join a node to an existing Docker Swarm as a worker or manager.\ndocker swarm leave: Make a node leave the Docker Swarm.\ndocker node ls: List all nodes in the Docker Swarm.","service-management#Service Management:":"docker service create: Create a new service.\ndocker service ls: List all services in the Docker Swarm.\ndocker service inspect: Inspect details of a specific service.\ndocker service update: Update configuration or scale a service.\ndocker service scale: Scale the number of replicas for a service.\ndocker service rm: Remove a service from the Docker Swarm.","task-and-container-management#Task and Container Management:":"docker ps: List running containers.\ndocker service ps: List tasks (containers) for a service.\ndocker logs: Fetch the logs of a container or service.\ndocker exec: Execute a command inside a running container.\ndocker stop: Stop a running container.\ndocker rm: Remove a container.","stack-deployment#Stack Deployment:":"docker stack deploy: Deploy a stack defined in a Docker Compose file.\ndocker stack ls: List all stacks in the Docker Swarm.\ndocker stack ps: List tasks (containers) of a stack.\ndocker stack services: List services in a stack.\ndocker stack rm: Remove a stack from the Docker Swarm.","secrets-management#Secrets Management:":"docker secret create: Create a new secret.\ndocker secret ls: List all secrets in the Docker Swarm.\ndocker secret inspect: Inspect details of a specific secret.\ndocker secret rm: Remove a secret from the Docker Swarm.","swarm-notes#Swarm-Notes":"Manager node and Worker Node Concept using Control Plane.\nManagers can become workers and Worker can become Manager.","emacs#Emacs":"Doom Emacs is a highly optimized and preconfigured Emacs distribution.\nIt aims to provide a faster and more efficient Emacs experience out of the box.\nDoom Emacs is built on top of the Emacs text editor, extending its functionality and providing a streamlined workflow.\nIt includes a curated set of popular packages and plugins, carefully chosen for their usefulness and performance.\nDoom Emacs utilizes a modular configuration system, allowing users to easily customize and extend their editor.\nIt supports various programming languages and provides language-specific configurations and features.\nDoom Emacs emphasizes a modal editing style, inspired by Vim, which can enhance productivity and speed.\nIt offers a comprehensive set of keybindings and shortcuts to navigate, edit, and manage files efficiently.\nDoom Emacs provides an integrated package manager, making it easy to install and update packages from various sources.\nIt has a strong and active community, with regular updates, documentation, and support available.","reference-4#Reference":"Documents -> https://github.com/doomemacs/doomemacs","elisp#Elisp":"Elisp (Emacs Lisp) is the programming language used for extending and customizing the Emacs text editor.\nIt is a dialect of Lisp, a family of programming languages known for its powerful and expressive syntax.\nElisp is a dynamically typed language, meaning variables do not have fixed types and can change at runtime.\nIt provides a rich set of built-in functions and macros for manipulating text, buffers, and files.\nElisp supports functional programming paradigms, including higher-order functions, closures, and recursion.\nIt has excellent integration with Emacs, allowing for seamless access to editor features and extensibility.\nElisp provides extensive support for interactive development, with a REPL (Read-Eval-Print Loop) for evaluating expressions in real-time.\nIt supports advanced features like dynamic scoping, macros for code generation, and the ability to redefine built-in functions.\nElisp has a robust and active community that contributes packages, libraries, and documentation for extending Emacs.\nEmacs Lisp is not limited to Emacs itself; it can also be used for scripting, automation, and general-purpose programming outside of the editor.","reference-5#Reference":"Tutorials ->\n- https://www.youtube.com/watch?v=ymSq4wHrqyU\n- https://learnxinyminutes.com/docs/elisp/","learning-inprogress#Learning-Inprogress":"","todo#Todo":"Docker Mastery with Kubernetes+Swarm\nchapter 70/99 -> All Videos before this chapter is useless\n\n\nDocker and Kubernetes\nChapter 123/170  -> All Videos before this chapter is useless\n\n\nRevisit\nhttps://www.docker.com/products/telepresence-for-docker/\n\n\nPending Docker dockes (No need of going through the docker guide not much to learn from there)\nhttps://docs.docker.com/config/daemon/start/\n\n\nTerraform Training start.\nhttps://docs.docker.com/config/daemon/start/\n\n\nStart Algorithms and practice some of it\nNginx tutorials\nMongod tutorials\nProstgres SQL\nLeetCode\nHackerRank\nCertifications https://training.linuxfoundation.org/it-career-roadmap/","practical#Practical":"Docker -> https://docs.docker.com/ [ Round Robin, Compose ]\nSwarm -> https://docs.docker.com/  [ OverlayNetwork, Routing Mesh, Stacks, Secrets in Swarm Services, Secrets with Stack, Swarm Life Cycle, Docker HeathCheck, Docker Registry, WebHooks,Create Automated Build,Repository Links,Certificats,Authentications,TLS,Private Repository, ]\nKubernetes ->\nTriver CI\nGO lang","finnish-language#Finnish Language":"","vim-cheat-sheet#Vim-Cheat-Sheet":"","navigation#Navigation":"h: Move left\nj: Move down\nk: Move up\nl: Move right\n0: Move to the beginning of the line\n$: Move to the end of the line\ngg:Move to the beginning of the file\nG: Move to the end of the file\nCtrl + f: Move forward one page\nCtrl + b: Move backward one page","editing#Editing":"i: Enter insert mode at the cursor\nI: Enter insert mode at the beginning of the line\na: Enter insert mode after the cursor\nA: Enter insert mode at the end of the line\no: Insert a new line below the current line\nO: Insert a new line above the current line\nx: Delete the character under the cursor\ndd: Delete the current line\nyy: Yank (copy) the current line\np: Paste the previously yanked or deleted text\nu: Undo the last change\nCtrl + r: Redo the last change","search-and-replace#Search-and-Replace":"/pattern: Search forward for \"pattern\"\n?pattern: Search backward for \"pattern\"\nn: Jump to the next occurrence of the search pattern\nN: Jump to the previous occurrence of the search pattern\n:%s/old/new/g: Replace all occurrences of \"old\" with \"new\" in the entire file\n:%s/old/new/gc: Replace all occurrences of \"old\" with \"new\" in the entire file with confirmation","saving-and-quitting#Saving-and-Quitting":":w: Save the file\n:q: Quit (close) the file\n:q!: Quit without saving (force quit)\n:wq or :x: Save and quit","other-useful-commands#Other-Useful-Commands":":set nu: Show line numbers\n:set nonu: Hide line numbers\n:set syntax=language: Enable syntax highlighting for a specific programming language\n:help keyword: Open Vim's built-in help for a specific keyword","masters-program#Masters-Program":"","finland#Finland":"","netherlands#Netherlands":"","germany#Germany":"","canada#Canada":"","usa#USA":"","norway#Norway":"","denmark#Denmark":"","newzeland#Newzeland":"","australia#Australia":"","create-github-hosted-react-frontend#Create-Github-hosted-React-Frontend":"Install node and npm inside devContainer by adding below propery to the devcontainer.json.\n\n\n\"features\": {\n\"ghcr.io/devcontainers/features/node:1\": {},\n\"ghcr.io/devcontainers-contrib/features/npm-package:1\": {}\n}\n\nCreate Svelte app\n\n\nnpm create svelte@latest ca-svelte-frontend --y\n>Skeleton project\n>Use Typescript\n>Select All\ncd ca-svelte-frontend\nnpm install\nnpm run dev\n\nDeploy to https://github.com/metonym/sveltekit-gh-pages.\nAdd homepage property to package.json \"homepage\": \"https://{username}.github.io/{repo-name}\".\nAdd a predeploy property and a deploy property to the scripts object in package.json .\n\n\n\"scripts\": {\n+   \"predeploy\": \"npm run build\",\n+   \"deploy\": \"gh-pages -d build\",\n\"build\": \"react-scripts build\",\n\"start\": \"react-scripts start\",\n\nrun command  npm run deploy -- -m \"Deploy React app to GitHub Pages\"\n\n\nUnder the hood, the predeploy script will build a distributable version of the React app and store it in a folder named build. Then, the deploy script will push the contents of that folder to a new commit on the gh-pages branch of the GitHub repository, creating that branch if it doesn't already exist.\n\nNavigate to the GitHub repository settings page> Code and automation> Pages\nupdate Source: Deploy from a branch, Branch: gh-pages, Folder: / (root).\nReference - > https://github.com/gitname/react-gh-pages#readme.\nMysite - > https://sunnyravindra.github.io/Cloud-Architect-Large-Enterprise-Practical-Guild/","next-js-frontend#Next-js-frontend":"Components. - > ReactApp/scr/components/MyFirstComponent/MyFirstComponent\nMake sure to use Uppercase letters while  importing Components\nJSX can have only one root element use  or <></>instead of div.\nCreate below folder structure.\n\n\nâ”œâ”€â”€ scss\nâ”‚   â”œâ”€â”€ main.scss\nâ”‚   â””â”€â”€ _variables.scss\nreference -> ReactApp/scr/scss\n5.  Create first react component\nconst MyFirstComponent = () => {\nreturn (\n<div>\n<div>MyFirstComponent</div>\n<div>MySecondComponent</div>\n</div>\n\n);\n}\nexport default MyFirstComponent;\n\ncall the MyFirstComponent in App.js\n\n\nimport React, { Fragment } from 'react';\nimport MyFirstComponent from './components/MyFirstComponent/MyFirstComponent';\nimport './scss/main.scss';\nconst App = () => {\nreturn (\n<Fragment>\n<MyFirstComponent></MyFirstComponent>\n<MyFirstComponent></MyFirstComponent>\n</Fragment>\n);\n}\nexport default App;\n\nAdd Props\n\n\nconst MyFirstComponent = (props) => {\nreturn (\n<div>\n<div>{props.Component1}</div>\n<div>{props.Component2}</div>\n</div>\n\n);\n}\nexport default MyFirstComponent;\n\nCall component with props in App.js\n\n\nconst App = () => {\nreturn (\n<Fragment>\n<MyFirstComponent Component1='firstComponent1' Component2='SecondComponent2'></MyFirstComponent>\n<MyFirstComponent Component1='firstComponent2' Component2='SecondComponent2'></MyFirstComponent>\n</Fragment>\n);\n}\n\nAdd Saas component file to the component folder.\n\n\nMyFirstComponent\nâ”‚       â”œâ”€â”€ MyFirstComponent.js\nâ”‚       â””â”€â”€ MyFirstComponent.scss\nReference ->\n10. Props Childeren Still To Understand.\n11. UseState\nconst [component,setcomponent] = useState(props.Component1);\n\nconst changeTitle = ()=> {\nsetcomponent('new componnent');\n};\n\nUse Next js instead on react\n\n\n\n\nStyles with Modlues and tailwindcss.\nnpm install tailwindcss\nDebugging, break points and React devtools.\nReact Form , form submission ,clearing the inputs , two way binding, Lifting state up\nSignle State Vs Mutiple states(53 and 54)\nRendering lists with Key property.\nTurnery expressions for conditional content\nPortal\nRefs -> good for reading values\nuseEffect hook","moving-to-next-js#Moving-to-Next-js":"Create next.js project and select default values\nnpm create-next-app\n\n\nclass myFirstClass{\nconstructor(){\nthis.constructor = 'constructor';\n}\nmyFirstArrowFunction = (message, constructor) => {\nvar myfirstVar = '4';\nconst myfirstConst = '5';\nconsole.log(message + myfirstVar + myfirstConst)\n}\n//Spread\nconst myFirstArray = [1,2,3,4,5];\nconst myFirstSpreadArray = [...myFirstArray];\nconsole.log(myFirstSpreadArray)\n//Rest\nmyFirstRestFunction = (...args) => {\nconsole.log(args)\n}\n//Array Destructuring\n[a,b, ,c]= myFirstArray;\nconsole.log(a);\nconsole.log(b);\nconsole.log(c);\n//Object Destructuring\n{c,d} = {name='sunny',age='cloud'}\nconsole.log(c);\nconsole.log(d);\n//objects are coped as reference -> practical pending\n//Map ->practical pending\n}","docker-compose#Docker-Compose":"","compose-important-points#Compose-Important-Points":"Definition of Multi-Container Applications: Docker Compose is a tool for defining and running multi-container Docker applications. It allows you to describe the services, networks, and volumes needed for your application in a declarative YAML file.\nCompose File: Docker Compose uses a YAML file called docker-compose.yml to define the configuration of your application. It includes services, networks, volumes, environment variables, and other options necessary to run your application.\nService: A service is a containerized component of your application. Each service is defined within the services section of the Compose file and can be built from a Dockerfile or use a pre-built image from a registry.\nNetworking: Docker Compose automatically creates a default network for your application, allowing services to communicate with each other using their service names as DNS names. You can also define custom networks for better isolation and control over your application's communication.\nVolumes: Volumes in Docker Compose allow you to persist and share data between containers or between a container and the host machine. Volumes can be defined in the volumes section of the Compose file.\nEnvironment Variables: Docker Compose allows you to set environment variables for your services using the environment key. This is useful for passing configuration values or connecting services together.\nContainer Orchestration: With Docker Compose, you can easily orchestrate the deployment, scaling, and management of multiple containers that make up your application. You can start, stop, and restart services as a group using simple commands.\nCommands: Docker Compose provides commands for managing your application, such as up to create and start containers, down to stop and remove containers, logs to view container logs, and more. These commands streamline the management of your multi-container application.\nExtensibility: Docker Compose supports extensions through the use of community-contributed tools called Compose plugins. These plugins provide additional functionalities like load balancing, monitoring, and service discovery.\nCompatibility: Docker Compose is compatible with both Docker Swarm mode and standalone Docker hosts, allowing you to deploy your applications on different Docker environments seamlessly.","reference-6#Reference":"version: '3'\nservices:\nweb:\nimage: nginx:latest\nports:\n- 80:80\nvolumes:\n- ./html:/usr/share/nginx/html","compose-file-structure#Compose-File-Structure":"version: '3'  # Compose file version\nservices:\nservice_name:  # Name of the service\nimage: image_name:tag  # Docker image and tag\nports:  # Port mapping\n- host_port:container_port\nvolumes:  # Volume mapping\n- host_path:container_path\nenvironment:  # Environment variables\n- KEY=VALUE\nnetworks:  # Networks to connect\n- network_name","compose-basic-commands#Compose-Basic-Commands":"docker-compose up: Create and start containers based on the Compose file.\ndocker-compose down: Stop and remove containers, networks, and volumes.\ndocker-compose start: Start existing containers.\ndocker-compose stop: Stop running containers.\ndocker-compose restart: Restart containers.\ndocker-compose pause: Pause containers.\ndocker-compose unpause: Unpause containers.\ndocker-compose ps: List containers managed by the Compose file.\ndocker-compose logs: View logs of containers.\ndocker-compose exec service_name command: Execute a command in a running container.\ndocker-compose build: Build or rebuild services.","compose-advanced-commands#Compose-Advanced-Commands":"docker-compose up -d: Start containers in detached mode.\ndocker-compose up --scale service_name=N: Scale a service to N instances.\ndocker-compose down --volumes: Remove containers, networks, and volumes (including named volumes).\ndocker-compose logs -f: Follow the logs of containers in real-time.\ndocker-compose exec -d service_name command: Execute a command in a running container in detached mode.\ndocker-compose config: Validate and view the Compose file.","compose-environment-variables#Compose-Environment-Variables":"Environment variables can be defined under the environment key within a service.\nVariables can be set individually or loaded from an external .env file using env_file configuration.","compose-volumes#Compose-Volumes":"Volumes can be defined under the volumes key within a service.\nVolumes allow data persistence and sharing between containers or between containers and the host.\nVolumes can be specified as named volumes, host paths, or anonymous volumes.","compose-networking#Compose-Networking":"By default, Compose creates a default network for the application.\nServices within the same Compose file can communicate using service names as DNS names.\nCustom networks can be defined using the networks key.","compose-extending-services#Compose-Extending-Services":"Compose allows extending services by defining a new service that inherits properties from a base service.\nInheritance is achieved by using the extends key within a service definition."}},"/Docker/Docker-Container":{"title":"Docker Container","data":{"":"Containers are runtime instances created from Docker images.\nThey provide a lightweight and isolated environment for running applications.\nContainers are created using the instructions specified in the Dockerfile and the layers from the corresponding Docker image.\nEach container has its own filesystem, processes, and network interface, which are isolated from the host system and other containers.\nContainers allow for consistent application execution across different computing environments.\nThey provide portability and reproducibility, ensuring that applications run reliably regardless of the underlying infrastructure.\nContainers are scalable and can be easily deployed and managed in orchestration platforms like Kubernetes.\nThey offer resource efficiency by sharing the host system's kernel, reducing overhead compared to traditional virtualization.\nContainers are ideal for deploying microservices-based architectures and enabling containerized application development.\nDocker provides tools and APIs to manage and orchestrate containers, making it easy to deploy, scale, and monitor containerized applications."}},"/Frontend Build Tools/Rollup":{"title":"Rollup","data":{"":"RollupRollup is a popular open-source JavaScript module bundler primarily designed for creating libraries or packages. It focuses on generating efficient and optimized bundles by leveraging the ES module syntax and tree-shaking capabilities.\nES Module-centric: Rollup is specifically designed to work with JavaScript modules that use the ES module syntax (import and export statements). It takes advantage of the native module support in modern browsers and aims to create optimized bundles without the need for additional runtime dependencies.\nTree Shaking: One of the key features of Rollup is tree shaking. It analyzes the imported and exported functions and variables across modules and eliminates the unused code during the bundling process. This helps reduce the final bundle size and improves the overall performance of the application.\nCode Splitting: Rollup supports code splitting, allowing developers to split their code into multiple chunks. This enables lazy loading and improves the initial loading time of an application by loading only the necessary code for a specific part of the application when required.\nSimplicity and Simplicity and Performance: Rollup focuses on simplicity and performance. It has a minimalistic and easy-to-understand configuration approach, making it less complex to set up compared to other bundlers. Additionally, Rollup's approach to bundling is known for its efficiency and fast build times.\nSupport for Multiple Formats: Rollup can generate bundles in various formats, including ES modules, CommonJS, AMD, and more. This flexibility allows developers to target different environments and use cases, whether it's for modern browsers, Node.js, or legacy systems.\nPlugin System: Rollup has a plugin-based architecture that allows developers to extend its functionality. Plugins can be used to handle non-JavaScript assets, apply transformations, optimize the code, or integrate with other tools or frameworks.\nIntegration with Build Tools and Frameworks: Rollup can be integrated into build tools and frameworks like webpack, allowing developers to combine the strengths of both tools. For example, webpack can handle tasks like development server, CSS bundling, and asset management, while Rollup can focus on JavaScript module bundling.\nLibrary-focused Approach: While Rollup can be used for full application bundling, it particularly shines when used for building libraries or packages. It optimizes the generated bundle by prioritizing small file sizes and clean module boundaries, making it a popular choice among library authors.\n\nRollup's emphasis on ES modules, tree shaking, simplicity, and performance makes it a compelling choice for projects that prioritize modern JavaScript module development and efficient code bundling. Its focus on creating optimized bundles aligns well with the needs of library authors and those seeking lightweight and performant web applications."}}}